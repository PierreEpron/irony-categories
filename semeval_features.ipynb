{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pierr\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from functools import reduce\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "from src.preprocessing import load_semeval_taskb\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-chat-hf', token=TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full, train, test = load_semeval_taskb()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Labels: Labels: 1 (ironic by clash), 2 (situational irony), 3 (other irony), 0 (not ironic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_distrib = pd.DataFrame(\n",
    "    [(x[0][0], x[0][1], x[1], round(x[1] / (full['split'] == x[0][0]).sum(), 3)) for x in full.groupby(['split', \"label_id\"])['label_id'].count().items()],\n",
    "    columns=['split', 'label_id', 'count', 'ratio']\n",
    ")\n",
    "\n",
    "sns.set(rc={'figure.figsize':(16, 6)})\n",
    "ax = sns.barplot(\n",
    "    data=label_distrib,\n",
    "    x='label_id',\n",
    "    y='count',\n",
    "    hue='split',\n",
    "    hue_order=['train', 'test']\n",
    ")\n",
    "ax.bar_label(\n",
    "    ax.containers[0], \n",
    "    labels=[f'{c:.0f} ({r})' for c, r in label_distrib[label_distrib.split == 'train'][['count', 'ratio']].values]\n",
    ") \n",
    "ax.bar_label(\n",
    "    ax.containers[1],     \n",
    "    labels=[f'{c:.0f} ({r})' for c, r in label_distrib[label_distrib.split == 'test'][['count', 'ratio']].values]\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama2 Tokenized text count distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full['llama2_tokens'] = full.text.apply(lambda x: [tokenizer.decode(tid) for tid in tokenizer.encode(x)[1:]])\n",
    "full['llama2_tokens_len'] = full.llama2_tokens.apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(full, x='label_id', y='llama2_tokens_len', hue='split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropped value count for token_len < 100\n",
    "full.groupby(['split', \"label_id\"])['label_id'].count() - full[full['llama2_tokens_len'] < 100].groupby(['split', \"label_id\"])['label_id'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop > 100 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = full[full['llama2_tokens_len'] < 100].reindex()\n",
    "full.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full['nltk_tokens'] = full.text.apply(lambda x: [t.strip().lower() for t in word_tokenize(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "\n",
    "en_stopwords = stopwords.words('english') + list(string.punctuation) + [\n",
    "    '...', \"''\", '``', 'http', \"'s\", \"n't\"\n",
    "]\n",
    "def get_counter(texts, stopwords=en_stopwords):\n",
    "    tokens = reduce(lambda a,b: a+b, texts, [])\n",
    "    tokens = list(filter(lambda x: x not in stopwords, tokens))\n",
    "    return Counter(tokens)\n",
    "\n",
    "full_tokens = get_counter(full.nltk_tokens.values)\n",
    "\n",
    "label_tokens = {}\n",
    "for label_id in sorted(full.label_id.unique()):\n",
    "    label_tokens[label_id] = get_counter(full[full.label_id == label_id].nltk_tokens.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"##### FULL #####\")\n",
    "print(full_tokens.most_common(20))\n",
    "for label_id in sorted(full.label_id.unique()):\n",
    "    print(f\"##### {label_id} #####\")\n",
    "    print(label_tokens[label_id].most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "irony",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
